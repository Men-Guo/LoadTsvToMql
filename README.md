#1. Introduction

##1.1 Project Goals
<br>Project goals include:</br>   
<br>1. Load Provided Product File using JAVA into MysQL DataBase </br>  
<br>2. Setup Apache SOLR instance</br>
<br>3. Load Products from DB into SOLR</br>
<br>4. Provide API to extract data out of SOLR</br>
   
##1.2 Major Functional Modules
   The major new functional modules and extensions to support the goals above may include:
   *    Product Verification
   *    Solr Module 
   *    Product Module 
   *    APIs for goal 1 and goal 4
   *    Convert tsv file to product entity

##1.3 Overall Results
The product.tsv table has 62574 products, database and solr documents have 62225 products. There are 349 products that are duplicated or invalid.
Search Api response within 100ms, average is 25ms.
#2. General Overview
   ##2.1 System overview

<br>Project is built on an open source framework. Source code is available using standard open source management tools such as github.</br>
<br>Project is hosted on a local machine.</br>
<br>Project’s development language is Java. Java is commonly integrated with Springboot that extends Java’s capabilities. Software developers must be familiar with this framework in order to maintain or build additional functionality into the application.</br>
<br>Project’s database is Mysql database. Since it is very popular and utilized across many open source applications.</br>
<br>Github is used as the software development platform. Github provides version control and source code management. Github is the largest host of source code in the world.</br>
<br>MyBatis is a first class persistence framework with support for custom SQL, stored procedures and advanced mappings.</br>
<br>Solr is the popular, blazing-fast, open source enterprise search platform built on Apache Lucene.</br>

##2.2 Sub-System
In summary, the existing system design includes the following sub-systems:

*   Sub-Systems:
    *   Spring Boot Application
    *   MySQL database
    *   Mybatis framework
    *   Solr
    *   Github Version and Source Code Control

There is no expectation that any of these systems will be changed or modified with the proposed system.

*   Dependencies:
    *   spring-boot-starter-web
    *   mybatis-spring-boot-starter
    *   mysql-connector-java
    *   mapper-spring-boot-starter
    *   lombok
    *   spring-boot-starter-test
    *   spring-boot-starter-data-solr

*   Plugins
    *   mybatis-generator-maven-plugin
    *   mybatis-generator-core

#3. Data
   ##3.1 MySQL table



##3.2 Class Design Details
*   Product - The main class mapped by data from file.
*   ProductExample - The class was generated by Mybatis auto generator, it contains lots of examples of service, I keep it as a source to implement service.  
*   SolrParam - The class was designed to receive data from users and it provides parameters for SolrQuery class.
*   SolarProduct - Map SolrDocument class back to product class inorder to return it back to frontend.
##3.3 Service Details

#4. APIs

##4.1 API for Load Data From A Path
*   */product/filepath?path = {value} - This api is used to input the path where the file store, it will send the path to method loadProductFromFile which will create product entities and store it into MySQL database.
##4.2 APIs for Solr Subsystem
*   */solr/uploadAll - This api will call Solr client to update all data in the database that in solr configuration, command will same as http://*:8983/solr/{db name}/dataimport?command=full-import
*   */solr/daleteAll - This api will delete all the documents from solr core, this api used when the data needs to update from the database.
*   */solr/search - This api will search for specific fields, default field is “Name” . This api need to send a Json body, for example:



#5. Project Process
   Assume : path stand for the localhost:port / servers:port
##5.1) Load Provided Product into MysQL DataBase
   *    Send a file path to API “path/product/filepath?path=product.tsv”
   *    The API will call the method loadProductDataFromAFile in  com.predictspring.controller
   *    Method loadProductDataFromAFile will call Product Service in package service,the product service, the service will call loadProductFromFile.
   *    The loadProductFromfile method will validate each line of file, if the line can convert to a product entity, it will add the entity into the database.
   *    After adding data from file, sent a request to api “path/solr/uploadAll”
   *    The api will call method uploadAll in SolrSearchController class, it will send a request with “path/solr/dih/dataimport?command=full-import” which will grab all data from the database and store it into the solr documents, now we can query data from solr.  
##5.2) Setup Apache SOLR instance
*   Created a single core (based on the required, the default will be a cloud)
    
*   Setup the data-config.xml 
    
*   Setup the managed-schema

*   Open the cmd, move the direction of solr/bin

*   run : solr.cmd start -e dih
*   solr will run at port 8983, and you can visit the admin dashboard at this port.
##5.3) Load Products from DB into SOLR
*   After adding data from file, sent a request to api “path/solr/uploadAll”
*   The api will call method uploadAll in SolrSearchController class, it will send a request with “path/solr/dih/dataimport?command=full-import” which will grab all data from the database and store it into the solr documents, now we can query data from solr.

##5.4) Provide API to extract data out of SOLR
*   Call API “path/solr/search” with body
*   It will return a RespBean with a List of Solr Product entities, which will return what the users want to search.

#6. Future Contingencies
##6.1) Database Improvement
*   Index - Frequently used as a query condition field should create an index
    
*   Fields that are updated very frequently are not suitable for index creation 
    
*   Split table technology (horizontal split, vertical split)
   
*   Optimize mysql configuration [Configure the maximum concurrent number my.ini, adjust the cache size]
   
*   Backup MySQL server
   
*   Master - Slave MySQL Cloud (CloudSQL)
    
##6.2) Data Configuration Auto Generator for Solr  
   
*   Since each time we need to hard code the field in schema-manage and data-configuration that map to the database table, we can create an auto generator that auto-creates the schema, it will save time for tons of reputational work.
   
## 6.3) Different Environments Configurations
   
*   For this project, I only use one environment, but for the scalability, it needs to change to a configuration that can change xml depending on the environments with one line code change. 
    *   For example:
        *   in dev environments : xxx.configuration.path = devConfig.xml 
            
        *   in test environments :xxx.configuration.path = testConfig.xml
    
        *   etc. 
            
##6.4) Could System on Reading/Writing
   
*   Now the load data into mysql database is very slow,  maybe we can use multiple threads that created the product entity and locked the add method to prevent a messed up in the database. 
    
*   For now, it's a single machine for writing and reading. In future, if search requirements become larger, we can implement a distributed/cloud system that improves the effectiveness, such as gRPC/Spring Cloud/ Dubbo, etc.
   
*   Also implement master - slave pattern, master write only and slave  for read. 
    
## 6.5) Could system for the solr
   
*   The Solr can be a cloud, with multiple ports. .
    
*   Increased efficiency.  
   
## 6.6) More Method APIs
   
*   CRUD for both Database and Solr 
    
*   Query ID in Solr
   
*   Delta update
   
*   Delete by ID
   
*   etc.  
